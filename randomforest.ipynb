{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "# from user_definition import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkContext & SparkSession.\n",
    "sc = SparkContext()\n",
    "ss = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'shardonnay697/shardonnay697.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+--------+------------+---------+-----------------+----------------------+----------+-----------------+---------+------------+--------+-------------+----------+-----------+\n",
      "|_c0|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|product_id|add_to_cart_order|reordered|product_name|aisle_id|department_id|department|      aisle|\n",
      "+---+--------+-------+--------+------------+---------+-----------------+----------------------+----------+-----------------+---------+------------+--------+-------------+----------+-----------+\n",
      "|  0| 2539329|      1|   prior|           1|        2|                8|                  null|     196.0|              1.0|      0.0|        Soda|    77.0|          7.0| beverages|soft drinks|\n",
      "+---+--------+-------+--------+------------+---------+-----------------+----------------------+----------+-----------------+---------+------------+--------+-------------+----------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = ss.read.csv(data_path, header=True, inferSchema=True)\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting strings to numeric values.\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def indexStringColumns(df, cols):\n",
    "    # variable newdf will be updated several times\n",
    "    newdf = df\n",
    "    \n",
    "    for c in cols:\n",
    "        #For each given colum, fits StringIndexerModel.\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-num\" suffix. \n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = indexStringColumns(df, ['ordered_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.filter('eval_set == \"prior\"').withColumnRenamed('ordered_true', 'label') \n",
    "test_df = df.filter('eval_set == \"train\"').withColumnRenamed('ordered_true', 'label') \n",
    "# .drop('_c0','eval_set', 'product_name', 'department', 'aisle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- order_number: integer (nullable = true)\n",
      " |-- order_dow: integer (nullable = true)\n",
      " |-- order_hour_of_day: integer (nullable = true)\n",
      " |-- days_since_prior_order: double (nullable = true)\n",
      " |-- product_id: double (nullable = true)\n",
      " |-- add_to_cart_order: double (nullable = true)\n",
      " |-- reordered: double (nullable = true)\n",
      " |-- aisle_id: double (nullable = true)\n",
      " |-- department_id: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "894"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.cache()\n",
    "# test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature vector and label column.\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=train_df.columns[0:-1]) #except the last col.\n",
    "carttrain = va.transform(train_df).select(\"features\", \"label\")\n",
    "\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=test_df.columns[0:-1]) #except the last col.\n",
    "cartvalid = va.transform(test_df).select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carttrain.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the data.\n",
    "carttrain.cache()\n",
    "cartvalid.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RandomForestClassifer and build a model using training dataset.\n",
    "rf = RandomForestClassifier(maxDepth=30)\n",
    "rfmodel = rf.fit(carttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using MulticlassClassificationEvaluator and test data.\n",
    "# Caclulate F1 score as evaluation metric.\n",
    "rfpredicts = rfmodel.transform(cartvalid)\n",
    "rfpredicts.show() # this is the DF that shows the model workings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator.evaluate(rfpredicts)\n",
    "print('F1 = %.4f' % f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpersist the datasets.\n",
    "carttrain.unpersist()\n",
    "cartvalid.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkContext & SparkSession.\n",
    "sc.stop()\n",
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
