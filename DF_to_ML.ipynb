{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:13:52.100532Z",
     "start_time": "2019-01-18T00:13:50.955393Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import asc, desc, dense_rank, col, when, count, avg, sum\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:14:08.125808Z",
     "start_time": "2019-01-18T00:13:52.100532Z"
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "ss = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:14:08.137776Z",
     "start_time": "2019-01-18T00:14:08.127803Z"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"order_id\", IntegerType(), True),\n",
    "StructField(\"user_id\", IntegerType(), True),\n",
    "StructField(\"eval_set\", StringType(), True),\n",
    "StructField(\"order_number\", IntegerType(), True),\n",
    "StructField(\"order_dow\", IntegerType(), True),\n",
    "StructField(\"order_hour_of_day\", IntegerType(), True),\n",
    "StructField(\"days_since_prior_order\", IntegerType(), True),\n",
    "StructField(\"product_id\", IntegerType(), True),\n",
    "StructField(\"add_to_cart_order\", IntegerType(), True),\n",
    "StructField(\"reordered\", IntegerType(), True),\n",
    "StructField(\"product_name\", StringType(), True),\n",
    "StructField(\"aisle_id\", IntegerType(), True),\n",
    "StructField(\"department_id\", IntegerType(), True),\n",
    "StructField(\"department\", StringType(), True),\n",
    "StructField(\"aisle\", StringType(), True)\n",
    "])\n",
    "# did not work with ss.read.csv\n",
    "# inferSchema accomplishes the same col types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change data_path variable to local consolidated_df.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/sankeerti/Documents/data/consolidated_df.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:14:37.076728Z",
     "start_time": "2019-01-18T00:14:08.140774Z"
    }
   },
   "outputs": [],
   "source": [
    "df = ss.read.csv(data_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:14:37.102318Z",
     "start_time": "2019-01-18T00:14:37.078349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- eval_set: string (nullable = true)\n",
      " |-- order_number: integer (nullable = true)\n",
      " |-- order_dow: integer (nullable = true)\n",
      " |-- order_hour_of_day: integer (nullable = true)\n",
      " |-- days_since_prior_order: double (nullable = true)\n",
      " |-- product_id: double (nullable = true)\n",
      " |-- add_to_cart_order: double (nullable = true)\n",
      " |-- reordered: double (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- aisle_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- aisle: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:23:54.841762Z",
     "start_time": "2019-01-18T00:23:09.365224Z"
    }
   },
   "outputs": [],
   "source": [
    "df.write.saveAsTable('Instacart') # saving as table for complex queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:15:08.178569Z",
     "start_time": "2019-01-18T00:14:37.103279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13307953"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique prior user_id-product_id combinations\n",
    "df.filter(\"eval_set == 'prior'\").groupby('user_id', 'product_id').count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:15:23.612535Z",
     "start_time": "2019-01-18T00:15:08.180563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1384617"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of train (order_id-product_id) rows\n",
    "df.filter(\"eval_set == 'train'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:15:39.262702Z",
     "start_time": "2019-01-18T00:15:23.614530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32434489"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of prior (order_id-product_id) rows\n",
    "df.filter(\"eval_set == 'prior'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~ Where I stopped running the code last night ~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:15:39.319577Z",
     "start_time": "2019-01-18T00:15:39.263698Z"
    }
   },
   "outputs": [],
   "source": [
    "last_orders_df = df.filter(\"eval_set == 'train'\").select('user_id', 'product_id') \\\n",
    "    .withColumnRenamed('user_id','user_id2') \\\n",
    "    .withColumnRenamed('product_id','product_id2')\n",
    "\n",
    "final_index_df = df.filter(\"eval_set == 'prior'\").select(\n",
    "    'user_id', 'product_id').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:16:30.640771Z",
     "start_time": "2019-01-18T00:15:39.320546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+-----------+\n",
      "|user_id|product_id|user_id2|product_id2|\n",
      "+-------+----------+--------+-----------+\n",
      "|      7|    8277.0|    null|       null|\n",
      "|      7|   27156.0|    null|       null|\n",
      "|      7|   40852.0|       7|    40852.0|\n",
      "|      8|   34358.0|    null|       null|\n",
      "|     14|   40540.0|    null|       null|\n",
      "|     18|   40723.0|    null|       null|\n",
      "|     27|    5322.0|    null|       null|\n",
      "|     31|   21131.0|    null|       null|\n",
      "|     31|   45104.0|    null|       null|\n",
      "|     32|   49478.0|    null|       null|\n",
      "|     38|   11078.0|    null|       null|\n",
      "|     41|   19678.0|    null|       null|\n",
      "|     42|    1263.0|    null|       null|\n",
      "|     52|   35561.0|    null|       null|\n",
      "|     58|   43115.0|    null|       null|\n",
      "|     61|    6187.0|    null|       null|\n",
      "|     63|   38061.0|    null|       null|\n",
      "|     71|   41408.0|    null|       null|\n",
      "|     79|   16616.0|      79|    16616.0|\n",
      "|     79|   28204.0|    null|       null|\n",
      "+-------+----------+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join leaves out new products never ordered before in train order\n",
    "df3 = final_index_df \\\n",
    "    .join(last_orders_df, \n",
    "          on=(final_index_df.user_id == last_orders_df.user_id2) \n",
    "          & (final_index_df.product_id == last_orders_df.product_id2), how=\"left\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:17:22.257963Z",
     "start_time": "2019-01-18T00:16:30.642768Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_target_df = df3.withColumn('ordered_true', df3.user_id2.isNotNull()) \\\n",
    "    .select('user_id','product_id','ordered_true')\n",
    "# feature_target_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:17:22.324785Z",
     "start_time": "2019-01-18T00:17:22.258960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[order_id: int, user_id: int, eval_set: string, order_number: int, order_dow: int, order_hour_of_day: int, days_since_prior_order: double, product_id: double, add_to_cart_order: double, reordered: double, product_name: string, aisle_id: string, department_id: string, department: string, aisle: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors_df = df.filter(\"eval_set == 'prior'\")\n",
    "\n",
    "priors_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of times a user ordered a product\n",
    "### Average days since prior order\n",
    "### Number of times user reordered each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:18:37.072535Z",
     "start_time": "2019-01-18T00:17:22.325783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, product_id: double, usr_prod_ct: bigint, num_reordered: double, avg_days_since_ord: double]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = priors_df.groupby('user_id', 'product_id') \\\n",
    "    .agg(count('order_id').alias('usr_prod_ct'),\n",
    "         avg('days_since_prior_order').alias('avg_days_since_ord_wnull'),\n",
    "         sum('reordered').alias('num_reordered'))\n",
    "\n",
    "features = features.withColumn('avg_days_since_ord', when(col('avg_days_since_ord_wnull').isNull(),365).otherwise(col('avg_days_since_ord_wnull'))) \\\n",
    "                    .drop('user_id2', 'product_id2','avg_days_since_ord_wnull')\n",
    "\n",
    "features.cache()\n",
    "# features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of times user ordered products in last 5 orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:20:14.663982Z",
     "start_time": "2019-01-18T00:18:56.358367Z"
    }
   },
   "outputs": [],
   "source": [
    "window = Window.partitionBy('user_id') \\\n",
    "                .orderBy(desc('order_number')) \\\n",
    "                .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "num_prod_ordl5 = priors_df.select('user_id', 'product_id', 'order_number',  \n",
    "                                  dense_rank().over(window).alias('rank')) \\\n",
    "                .filter(col('rank') <= 5) \\\n",
    "                .groupby('user_id', 'product_id') \\\n",
    "                .count() \\\n",
    "                .withColumnRenamed('user_id','user_id2') \\\n",
    "                .withColumnRenamed('product_id','product_id2')\n",
    "\n",
    "features = features.join(num_prod_ordl5, \n",
    "          on=(features.user_id == num_prod_ordl5.user_id2) \n",
    "          & (features.product_id == num_prod_ordl5.product_id2), how=\"left\")\n",
    "\n",
    "features = features.withColumn('num_prod_ordl5', when(col('count').isNull(),0).otherwise(col('count'))) \\\n",
    "                    .drop('user_id2', 'product_id2','count')\n",
    "\n",
    "\n",
    "# features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio of orders user ordered products in last 5 orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:22:29.253615Z",
     "start_time": "2019-01-18T00:20:14.666972Z"
    }
   },
   "outputs": [],
   "source": [
    "ratio_prod_ordl5 = priors_df.select('user_id', 'product_id', 'order_number',  \n",
    "          dense_rank().over(window).alias('rank')) \\\n",
    "                .filter(col('rank') <= 5) \\\n",
    "                .groupby('user_id', 'product_id') \\\n",
    "                .agg((count('order_number')/5).alias('ratio_wnull'))\\\n",
    "                .withColumnRenamed('user_id','user_id2') \\\n",
    "                .withColumnRenamed('product_id','product_id2')\n",
    "\n",
    "features = features.join(ratio_prod_ordl5, \n",
    "          on=(features.user_id == ratio_prod_ordl5.user_id2) \n",
    "          & (features.product_id == ratio_prod_ordl5.product_id2), how=\"left\")\n",
    "\n",
    "features = features.withColumn('last5_ratio', when(col('ratio_wnull').isNull(),0).otherwise(col('ratio_wnull'))) \\\n",
    "                    .drop('user_id2', 'product_id2','ratio_wnull')\n",
    "\n",
    "# features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of orders since a user last ordered a given item\n",
    "done by generating chrononological order_num from order_id, and returns max order_num (grouped by user) - max order_num (grouped by user and product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:39:41.090139Z",
     "start_time": "2019-01-18T00:39:41.017306Z"
    }
   },
   "outputs": [],
   "source": [
    "num_ords_since_last = ss.sql(\"select distinct product_id as product_id2, user_id as user_id2,\\\n",
    "        max(order_num) over (partition by user_id) - max(order_num) over (partition by user_id, product_id) as num_ords_since_last from\\\n",
    "        (select Instacart.order_id, Instacart.user_id, Instacart.product_id, rhs.order_num\\\n",
    "        from Instacart\\\n",
    "        left join\\\n",
    "        (select order_id, user_id, row_number() over (partition by user_id order by order_id) as order_num from\\\n",
    "        (select distinct order_id, user_id from Instacart where eval_set = 'prior') as iq) as rhs\\\n",
    "        on Instacart.order_id=rhs.order_id and Instacart.user_id=rhs.user_id\\\n",
    "        where eval_set = 'prior') as iq2\")\n",
    "\n",
    "features = features.join(num_ords_since_last, \n",
    "          on=(features.user_id == num_ords_since_last.user_id2) \n",
    "          & (features.product_id == num_ords_since_last.product_id2), how=\"left\")\n",
    "\n",
    "features = features.drop('user_id2', 'product_id2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:47:47.282538Z",
     "start_time": "2019-01-18T00:39:54.758803Z"
    }
   },
   "outputs": [],
   "source": [
    "# features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate of user item reorder: # of reorders of an item / # of orders since first time ordering item.\n",
    "Get max(order_num) grouped by user_id, then min(order_num) grouped by user_id and product, subtract the two to get number of orders since first purchase of an item. Then sum(reordered) grouped by item, user to get the number of times an item was reordered by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T00:47:47.390249Z",
     "start_time": "2019-01-18T00:47:47.286530Z"
    }
   },
   "outputs": [],
   "source": [
    "reorder_rate = ss.sql(\"select product_id as product_id2, user_id as user_id2, \\\n",
    "        num_reorders/orders_since_first as reorder_rate_wnull from\\\n",
    "        (select distinct product_id, user_id,\\\n",
    "        max(order_num) over (partition by user_id) - min(order_num) over (partition by user_id, product_id) as orders_since_first,\\\n",
    "        sum(reordered) over (partition by user_id, product_id) as num_reorders from\\\n",
    "        (select Instacart.order_id, Instacart.user_id, Instacart.product_id, Instacart.reordered, rhs.order_num\\\n",
    "        from Instacart\\\n",
    "        left join\\\n",
    "        (select order_id, user_id, row_number() over (partition by user_id order by order_id) as order_num from\\\n",
    "        (select distinct order_id, user_id from Instacart where eval_set = 'prior') as iq) as rhs\\\n",
    "        on Instacart.order_id=rhs.order_id and Instacart.user_id=rhs.user_id\\\n",
    "        where eval_set = 'prior') as iq2) as iq3\")\n",
    "\n",
    "features = features.join(reorder_rate, \n",
    "          on=(features.user_id == reorder_rate.user_id2) \n",
    "          & (features.product_id == reorder_rate.product_id2), how=\"left\")\n",
    "\n",
    "features = features.withColumn('reorder_rate', when(col('reorder_rate_wnull').isNull(),0).otherwise(col('reorder_rate_wnull'))) \\\n",
    "                    .drop('reorder_rate_wnull')\n",
    "\n",
    "features = features.drop('user_id2', 'product_id2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T01:18:03.480999Z",
     "start_time": "2019-01-18T01:07:20.780551Z"
    }
   },
   "outputs": [],
   "source": [
    "features = features.withColumn('reorder_rate_new', when(col('reorder_rate').isNull(),0).otherwise(col('reorder_rate'))) \\\n",
    "                    .drop('reorder_rate')\n",
    "# features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T01:31:39.825786Z",
     "start_time": "2019-01-18T01:21:15.559527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13307953"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+-------------+------------------+--------------+-----------+-------------------+-------------------+\n",
      "|user_id|product_id|usr_prod_ct|num_reordered|avg_days_since_ord|num_prod_ordl5|last5_ratio|num_ords_since_last|   reorder_rate_new|\n",
      "+-------+----------+-----------+-------------+------------------+--------------+-----------+-------------------+-------------------+\n",
      "|      7|    8277.0|          3|          2.0|11.666666666666666|             1|        0.2|                  2|0.10526315789473684|\n",
      "|      7|   27156.0|          1|          0.0|               7.0|             1|        0.2|                  4|                0.0|\n",
      "|      7|   40852.0|         13|         12.0| 12.76923076923077|             3|        0.6|                  0|  0.631578947368421|\n",
      "+-------+----------+-----------+-------------+------------------+--------------+-----------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+\n",
      "|user_id|product_id|ordered_true|\n",
      "+-------+----------+------------+\n",
      "|      7|    8277.0|       false|\n",
      "|      7|   27156.0|       false|\n",
      "|      7|   40852.0|        true|\n",
      "+-------+----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_target_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13307953"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_target_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_target_df2 = feature_target_df.withColumnRenamed('user_id', 'user_id2')\\\n",
    "                                      .withColumnRenamed('product_id', 'product_id2')\n",
    "final_df = features.join(feature_target_df2, \n",
    "          on=(features.user_id == feature_target_df2.user_id2) \n",
    "          & (features.product_id == feature_target_df2.product_id2), how=\"left\")\n",
    "final_df = final_df.drop('user_id2', 'product_id2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+-------------+------------------+--------------+-----------+-------------------+-------------------+------------+\n",
      "|user_id|product_id|usr_prod_ct|num_reordered|avg_days_since_ord|num_prod_ordl5|last5_ratio|num_ords_since_last|   reorder_rate_new|ordered_true|\n",
      "+-------+----------+-----------+-------------+------------------+--------------+-----------+-------------------+-------------------+------------+\n",
      "|      7|    8277.0|          3|          2.0|11.666666666666666|             1|        0.2|                  2|0.10526315789473684|       false|\n",
      "|      7|   27156.0|          1|          0.0|               7.0|             1|        0.2|                  4|                0.0|       false|\n",
      "|      7|   40852.0|         13|         12.0| 12.76923076923077|             3|        0.6|                  0|  0.631578947368421|        true|\n",
      "+-------+----------+-----------+-------------+------------------+--------------+-----------+-------------------+-------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- product_id: double (nullable = true)\n",
      " |-- usr_prod_ct: long (nullable = false)\n",
      " |-- num_reordered: double (nullable = true)\n",
      " |-- avg_days_since_ord: double (nullable = true)\n",
      " |-- num_prod_ordl5: long (nullable = true)\n",
      " |-- last5_ratio: double (nullable = true)\n",
      " |-- num_ords_since_last: integer (nullable = true)\n",
      " |-- reorder_rate_new: double (nullable = true)\n",
      " |-- ordered_true: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting strings to numeric values.\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def indexStringColumns(df, cols):\n",
    "    # variable newdf will be updated several times\n",
    "    newdf = df\n",
    "    \n",
    "    for c in cols:\n",
    "        #For each given colum, fits StringIndexerModel.\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-num\" suffix. \n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: The input column ordered_true must be either string type or numeric type, but got BooleanType.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/MSDS694/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MSDS694/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o274.fit.\n: java.lang.IllegalArgumentException: requirement failed: The input column ordered_true must be either string type or numeric type, but got BooleanType.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.feature.StringIndexerBase$class.validateAndTransformSchema(StringIndexer.scala:86)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:109)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:152)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:135)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:109)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-f6d840905384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexStringColumns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ordered_true'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-cf25a89c2ad5>\u001b[0m in \u001b[0;36mindexStringColumns\u001b[0;34m(df, cols)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#For each given colum, fits StringIndexerModel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-num\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m#Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#and then drops the original columns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MSDS694/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/miniconda3/envs/MSDS694/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MSDS694/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MSDS694/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MSDS694/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: The input column ordered_true must be either string type or numeric type, but got BooleanType.'"
     ]
    }
   ],
   "source": [
    "dfnum = indexStringColumns(final_df, ['ordered_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
